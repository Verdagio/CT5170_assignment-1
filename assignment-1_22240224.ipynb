{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CT5170 - Principles of Machine Learning\n",
    "### Assignment 1\n",
    "##### Daniel Verdejo - 22240224\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: _Have a look at the given data, understand the problem based on the dependent variable and select a machine learning category that can solve the task/problem.  Briefly explain why do you think it is the correct ML category for this problem?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "The data provided is a subset of the Iris dataset which typically includes 3 species. We have both a training and test dataset for our neural network to consume. \n",
    "\n",
    "To solve the problem we will use a supervised learning classification task as we are provided 80 samples in total in a training dataset. \n",
    "\n",
    "Classification will enable us to organise our data in categorical sense based off predictions made about each sample the model is given. \n",
    "\n",
    "We first train the model with a dataset of known data, consisting of 2 classes with 40 samples per class (as seen below), each sample containing 4 attributes and the target label. The attributes are the sepal width and length (in cm), and the petal width and length (in cm). \n",
    "\n",
    "Once trained we will then need to feed it new data (data it has not yet seen) to measure its performance.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11e02667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data:  [['setosa' '40']\n",
      " ['virginica' '40']]\n",
      "\n",
      "test data:  [['setosa' '10']\n",
      " ['virginica' '10']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danny\\AppData\\Local\\Temp\\ipykernel_11448\\1192729505.py:9: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  attributes = np.array(training_samples)[:,:4].astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import keras as kr\n",
    "\n",
    "training_samples = list(csv.reader(open('./plant-data/plant-train.csv')))[1:]\n",
    "testing_samples = list(csv.reader(open('./plant-data/plant-test.csv')))[1:]\n",
    "\n",
    "# the attributes will be our inputs - sepal width, sepal length, petal width, petal length \n",
    "attributes = np.array(training_samples)[:,:4].astype(np.float)\n",
    "\n",
    "# The classes will be our outputs\n",
    "training_classes = np.array(training_samples)[:,4]\n",
    "testing_classes = np.array(testing_samples)[:,4]\n",
    "\n",
    "uniq_training_classes, output_indices, training_counts = np.unique(training_classes, return_counts=True, return_inverse=True)\n",
    "print('training data: ', np.asarray((uniq_training_classes, training_counts)).T)\n",
    "\n",
    "uniq_test_classes, test_counts = np.unique(testing_classes, return_counts=True)\n",
    "print('\\ntest data: ', np.asarray((uniq_test_classes, test_counts)).T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eabe9a9",
   "metadata": {},
   "source": [
    "2: _Explore and report the data and its distribution among training and testing data. Can we call it imbalanced dataset, explain your answer (yes/no) briefly?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b41bc4",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "No, we cannot say the provided dataset is imbalanced. As we can see above, both the training and testing datasets show a 50:50 ratio for each class. The dataset is perfectly balanced with no specific class showing a majority or minority in either the training or testing datasets.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da31b8",
   "metadata": {},
   "source": [
    "3: _Research and write down about open-source machine learning package that are freely available, and select one that you think will be good and easy for this task. Your report should include a short overview of the main features of the package you have chosen._ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92b3fa",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Some of the most popular machine learing libraries for python are as follows:\n",
    "\n",
    "**TensorFlow** - Developed by Google, written in Python and C++, it offers a free and open-source library used for artificial intelligence and machine learning applications. The library offers both a high-level and low-level API to users to enable creating machine leanring models which can be run on many different host devices including, cloud, desktop, mobile or even on edge devices. It offers tools to validate and transform large datasets, and tools to discover and remove bias in data to improve outcomes on models. Tensoflow offers in depth API documentation and learning materials for users of all experience levels.\n",
    "\n",
    "**Keras** - Is a machine learning API written purely in Python, which runs on top of the TensorFlow platform. It offers an easy to use high-level API which abstracts some of the more complex functionality of TensorFlow. Keras offers a quick and easy way to get up and running for smaller projects.\n",
    "\n",
    "**PyTorch** - Developed by the Linux Foundation and Meta AI (a branch of Meta, formerly Facebook) was released in September 2016. The library is written in Python, CUDA and C++, and offers a machine learning framework based on the Torch library. It can be used to develop machine learning applications and also offers capabilities for the creation of REST API endpoint to ease application integration. It only offers low level apis so it can be more difficult to use than Keras.\n",
    "\n",
    "\n",
    "The machine learning library that I will be using here is [Keras](https://keras.io/). Its ease of use makes carrying out tasks like the one we are tackling today quick and easy. \n",
    "\n",
    "Using the Keras library over something like the TensorFlow or PyTorch libraries will reduce the amount of effort on my part to build the neural network, and should also be fairly easy to read and understand for you, the reader. As this dataset is quite small and relatively trivial using one of the other options, while they may operate faster, would add an unnecessary level of complexity as we should not need to dig into the lower level apis, or spend a lot of time debugging our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbd8609",
   "metadata": {},
   "source": [
    "4: _In order to use the dataset (Plant-dataset) supplied below, you might need to do some work to prepare it for input into the ML package, depending on the ML category requirements. Document any data preparation (e.g. normalisation) steps in your report._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfd809e",
   "metadata": {},
   "source": [
    "Some preparation of the dataset is required, we first got our attributes above like so: `attributes = np.array(training_samples)[:,:4].astype(np.float)` these our inputs.\n",
    "\n",
    "Next we got our classes: `training_classes = np.array(training_samples)[:,4]` these will be our outputs.\n",
    "\n",
    "Above we used `uniq_training_classes, inverse, training_counts = np.unique(training_classes, return_counts=True, return_inverse=True)` for  the training dataset to view how the data is distributed among the 2 class in both datasets. We also got the indicies of the unique array. This consists of 0 and 1s which points us to the index for each output. As we can see below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b5cad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(output_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b834d49",
   "metadata": {},
   "source": [
    "5: _From the ML package, select two different algorithms from the category you selected and apply to the dataset. In your report, include a clear description of both algorithms. Ensure that you acknowledge all of your sources of information.\n",
    "Report the results with and without normalisation of the data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a15871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential model\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "#add our initial layer with an input for each of our attributes (4), and a hidden layer with 16 nodes\n",
    "model.add(kr.layers.Dense(16, input_shape=(4,)))\n",
    "\n",
    "# apply an activation function to the layer\n",
    "model.add(kr.layers.Activation('sigmoid'))\n",
    "\n",
    "# add our output layer with one node for each class\n",
    "model.add(kr.layers.Dense(2))\n",
    "\n",
    "# apply an activation function to the output layer\n",
    "model.add(kr.layers.Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ebffeb",
   "metadata": {},
   "source": [
    "6: _Train and test your chosen algorithms using the training set provided in plant-train.csv. You should then test your trained models using the test set provided in plant-test.csv. Report on the results with appropriate performance metric e.g. accuracy that you consider best for each model on the training set and on the test set. Also include details of the classification models constructed â€“ these may include graphics if appropriate._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967fd72",
   "metadata": {},
   "source": [
    "\n",
    "7: _Discuss in your report whether the two models give very similar or significantly different results, and why._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "311f9ad72853ce38d00a12521a098bf105a13a75490bd1b1a64bc7cd30e01863"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
